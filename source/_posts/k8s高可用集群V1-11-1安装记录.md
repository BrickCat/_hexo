---
title: k8s高可用集群V1.11.1安装记录
date: 2018-09-13 10:23:54
tags: [kubernetes,docker,容器,k8s]
categories: [容器技术,kubernetes,docker,k8s]
---
### 实验环境

hostname | IP | 内存 | 职责
---|---|---|---
k8s-vip | 192.168.1.110 | —— | VIP（虚拟IP）
k8s-master1 | 192.168.1.111 | 4G | Master,Keepalived,HAProxy
k8s-master2 | 192.168.1.112 | 4G | Master,Keepalived,HAProxy
k8s-master3 | 192.168.1.113 | 4G | Master,Keepalived,HAProxy
k8s-slave1 | 192.168.1.114 | 4G |  Worker
k8s-slave1 | 192.168.1.115 | 4G |  Worker
<!-- more -->
### 环境准备

##### 1. 配置hosts信息（所有节点）
```
cat <<EOF >> /etc/hosts
192.168.1.110 k8s-vip
192.168.1.111 k8s-master1
192.168.1.112 k8s-master2
192.168.1.113 k8s-master3
192.168.1.114 k8s-slave1
192.168.1.115 k8s-slave2
EOF
```
##### 2. 安装Docker （所有节点）
安装Docker，步骤略可参考http://www.ebanban.com/?p=496

##### 3. 在所有Master节点上输入以下环境变量，主机名和IP信息根据自己的实际的情况进行修改（#台Master节点）
```
export KUBECONFIG=/etc/kubernetes/admin.conf
export LOAD_BALANCER_DNS=k8s-vip
export LOAD_BALANCER_PORT=8443
export CP0_HOSTNAME=k8s-master1
export CP1_HOSTNAME=k8s-master2
export CP2_HOSTNAME=k8s-master3
export VIP_IP=192.168.1.110
export CP0_IP=192.168.1.111
export CP1_IP=192.168.1.112
export CP2_IP=192.168.1.113
```

##### 4. 关闭防火墙、关闭swap、关闭SELinux、调整内核参数(所有节点)
```
sudo systemctl stop firewalld
sudo systemctl disable firewalld
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
sudo setenforce 0
sed -i 's/SELINUX=permissive/SELINUX=disabled/' /etc/sysconfig/selinux
cat <<EOF > /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system
```
##### 5.准备镜像（所有节点）
```
# 创建脚本加入如下内容
#!/bin/bash
images=(kube-proxy-amd64:v1.11.1 kube-scheduler-amd64:v1.11.1 kube-controller-manager-amd64:v1.11.1 kube-apiserver-amd64:v1.11.1
etcd-amd64:3.2.18 pause-amd64:3.1 kubernetes-dashboard-amd64:v1.8.3 k8s-dns-sidecar-amd64:1.14.9 k8s-dns-kube-dns-amd64:1.14.9
k8s-dns-dnsmasq-nanny-amd64:1.14.9 )
for imageName in ${images[@]} ; do
docker pull mirrorgooglecontainers/$imageName
docker tag mirrorgooglecontainers/$imageName k8s.gcr.io/$imageName
docker rmi mirrorgooglecontainers/$imageName
done
docker tag da86e6ba6ca1 k8s.gcr.io/pause:3.1
docker pull coredns/coredns:1.1.3
docker tag coredns/coredns:1.1.3 k8s.gcr.io/coredns:1.1.3
docker rmi coredns/coredns:1.1.3
```
```
# 添加运行权限
chmod -R 777 ./xxx.sh

# 执行脚本
./xxx.sh
```
##### 6. 安装、配置kubelet （所有节点）
```
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
yum install -y kubelet-1.11.1 kubeadm-1.11.1 kubectl-1.11.1
systemctl enable kubelet
```
##### 7.准备SSH Keys（任意Master节点）
```
# 生成SSH Key（通常在第一台Master上操作，可以在终端上操作）
ssh-keygen -t rsa -b 2048 -f /root/.ssh/id_rsa -N ""

# 将SSH Key复制给其他主机
for host in {$CP0_HOSTNAME,$CP1_HOSTNAME,$CP2_HOSTNAME}; do ssh-copy-id $host; done $host; done
```
##### 8. 部署第一个keepalived （第一个Master节点）
**keepalived用于生产浮动的虚拟IP，并将浮动IP分配给优先级最高且haproxy正常运行的节点
在第一台Master上配置和启动keepalived，若网卡名称不为示例中的eth0则改为对应名称**

```
# 安装 部署第一个keepalived
yum install -y keepalived curl psmisc && systemctl enable keepalived

# 自定义配置
cat << EOF > /etc/keepalived/keepalived.conf
vrrp_script haproxy-check {
    script "killall -0 haproxy"
    interval 2
    weight 20
} 
vrrp_instance haproxy-vip {
    state BACKUP
    priority 102
    interface ens33
    virtual_router_id 51
    advert_int 3
    unicast_src_ip $CP0_IP
    unicast_peer {
        $CP1_IP
        $CP2_IP
    }
    virtual_ipaddress {
        $VIP_IP
    }
    track_script {
        haproxy-check weight 20
    }
}
EOF

# 启动 keepalived
systemctl start keepalived
```
##### 9. 部署第二个keepalived （第二个Master节点）
```
# 安装 部署第一个keepalived
yum install -y keepalived curl psmisc && systemctl enable keepalived

# 自定义配置
cat << EOF > /etc/keepalived/keepalived.conf
vrrp_script haproxy-check {
    script "killall -0 haproxy"
    interval 2
    weight 20
}
vrrp_instance haproxy-vip {
    state BACKUP
    priority 101
    interface ens33
    virtual_router_id 51
    advert_int 3
    unicast_src_ip $CP1_IP
    unicast_peer {
        $CP0_IP
        $CP2_IP
    }
    virtual_ipaddress {
        $VIP_IP
    }
    track_script {
        haproxy-check weight 20
    }
}
EOF

# 启动 keepalived
systemctl start keepalived
```

##### 10. 部署第三个keepalived （第三个Master节点）
```
# 安装 部署第一个keepalived
yum install -y keepalived curl psmisc && systemctl enable keepalived

# 自定义配置
cat << EOF > /etc/keepalived/keepalived.conf
vrrp_script haproxy-check {
    script "killall -0 haproxy"
    interval 2
    weight 20
}
vrrp_instance haproxy-vip {
    state BACKUP
    priority 100
    interface ens33
    virtual_router_id 51
    advert_int 3
    unicast_src_ip $CP2_IP
    unicast_peer {
        $CP0_IP
        $CP1_IP
    }
    virtual_ipaddress {
        $VIP_IP
    }
    track_script {
        haproxy-check weight 20
    }
}
EOF

# 启动 keepalived
systemctl start keepalived
```
##### 11. 部署HAProxy （所有Master节点）
```
# 安装HAproxy
yum install -y haproxy && systemctl enable haproxy

# 自定义配置文件
cat << EOF > /etc/haproxy/haproxy.cfg
global
  log 127.0.0.1 local0
  log 127.0.0.1 local1 notice
  tune.ssl.default-dh-param 2048

defaults
  log global
  mode http
  option dontlognull
  timeout connect 5000ms
  timeout client  600000ms
  timeout server  600000ms

listen stats
    bind :9090
    mode http
    balance
    stats uri /haproxy_stats
    stats auth admin:admin
    stats admin if TRUE

frontend kube-apiserver-https
   mode tcp
   bind :8443
   default_backend kube-apiserver-backend

backend kube-apiserver-backend
    mode tcp
    balance roundrobin
    stick-table type ip size 200k expire 30m
    stick on src
    server k8s-master1 192.168.1.111:6443 check
    server k8s-master2 192.168.1.112:6443 check
    server k8s-master3 192.168.1.113:6443 check
EOF

# 启动 HAproxy
systemctl start haproxy

```

###  安装kubernetes


##### 1. 配置第一个Master节点（第一个Master节点）

```
cat >kubeadm-master.config<<EOF
apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
kubernetesVersion: v1.11.1

apiServerCertSANs:
- "k8s-master1"
- "k8s-master2"
- "k8s-master3"
- "192.168.1.111"
- "192.168.1.112"
- "192.168.1.113"
- "192.168.1.110"
- "127.0.0.1"

api:
  advertiseAddress: $CP0_IP
  controlPlaneEndpoint: 192.168.1.110:8443

etcd:
  local:
    extraArgs:
      listen-client-urls: "https://127.0.0.1:2379,https://$CP0_IP:2379"
      advertise-client-urls: "https://$CP0_IP:2379"
      listen-peer-urls: "https://$CP0_IP:2380"
      initial-advertise-peer-urls: "https://$CP0_IP:2380"
      initial-cluster: "$CP0_HOSTNAME=https://$CP0_IP:2380"
    serverCertSANs:
      - $CP0_HOSTNAME
      - $CP0_IP
    peerCertSANs:
      - $CP0_HOSTNAME
      - $CP0_IP

controllerManagerExtraArgs:
  node-monitor-grace-period: 10s
  pod-eviction-timeout: 10s

networking:
  podSubnet: 10.244.0.0/16
EOF

# 初始化
# 注意保存返回的 join 命令
kubeadm init --config kubeadm-master.config

# 打包ca相关文件上传至其他master节点
cd /etc/kubernetes && tar cvzf k8s-key.tgz admin.conf pki/ca.* pki/sa.* pki/front-proxy-ca.* pki/etcd/ca.*

scp k8s-key.tgz k8s-master2:~/
scp k8s-key.tgz k8s-master3:~/
ssh k8s-master2 'tar xf k8s-key.tgz -C /etc/kubernetes/'
ssh k8s-master3 'tar xf k8s-key.tgz -C /etc/kubernetes/'
```
##### 2. 配置第二个Master节点（第二个Master节点）
```
cat >kubeadm-master.config<<EOF
apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
kubernetesVersion: v1.11.1

apiServerCertSANs:
- "k8s-master1"
- "k8s-master2"
- "k8s-master3"
- "192.168.1.111"
- "192.168.1.112"
- "192.168.1.113"
- "192.168.1.110"
- "127.0.0.1"

api:
  advertiseAddress: $CP1_IP
  controlPlaneEndpoint: 192.168.1.110:8443

etcd:
  local:
    extraArgs:
      listen-client-urls: "https://127.0.0.1:2379,https://$CP1_IP:2379"
      advertise-client-urls: "https://$CP1_IP:2379"
      listen-peer-urls: "https://$CP1_IP:2380"
      initial-advertise-peer-urls: "https://$CP1_IP:2380"
      initial-cluster: "$CP0_HOSTNAME=https://$CP0_IP:2380,$CP1_HOSTNAME=https://$CP1_IP:2380"
      initial-cluster-state: existing
    serverCertSANs:
      - $CP1_HOSTNAME
      - $CP1_IP
    peerCertSANs:
      - $CP1_HOSTNAME
      - $CP1_IP

controllerManagerExtraArgs:
  node-monitor-grace-period: 10s
  pod-eviction-timeout: 10s

networking:
  podSubnet: 10.244.0.0/16
EOF

# 配置kubelet
kubeadm alpha phase certs all --config kubeadm-master.config
kubeadm alpha phase kubelet config write-to-disk --config kubeadm-master.config
kubeadm alpha phase kubelet write-env-file --config kubeadm-master.config
kubeadm alpha phase kubeconfig kubelet --config kubeadm-master.config
systemctl restart kubelet

# 添加etcd到集群中
KUBECONFIG=/etc/kubernetes/admin.conf kubectl exec -n kube-system etcd-${CP0_HOSTNAME} -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://${CP0_IP}:2379 member add ${CP1_HOSTNAME} https://${CP1_IP}:2380
kubeadm alpha phase etcd local --config kubeadm-master.config

# 部署
kubeadm alpha phase kubeconfig all --config kubeadm-master.config
kubeadm alpha phase controlplane all --config kubeadm-master.config
kubeadm alpha phase mark-master --config kubeadm-master.config
```
##### 3. 部署第三个Master节点 （第三个Master节点）
```
cat >kubeadm-master.config<<EOF
apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
kubernetesVersion: v1.11.1

apiServerCertSANs:
- "k8s-master1"
- "k8s-master2"
- "k8s-master3"
- "192.168.1.111"
- "192.168.1.112"
- "192.168.1.113"
- "192.168.1.110"
- "127.0.0.1"

api:
  advertiseAddress: $CP2_IP
  controlPlaneEndpoint: 192.168.1.110:8443

etcd:
  local:
    extraArgs:
      listen-client-urls: "https://127.0.0.1:2379,https://$CP2_IP:2379"
      advertise-client-urls: "https://$CP2_IP:2379"
      listen-peer-urls: "https://$CP2_IP:2380"
      initial-advertise-peer-urls: "https://$CP2_IP:2380"
      initial-cluster: "$CP0_HOSTNAME=https://$CP0_IP:2380,$CP1_HOSTNAME=https://$CP1_IP:2380,$CP2_HOSTNAME=https://$CP2_IP:2380"
      initial-cluster-state: existing
    serverCertSANs:
      - $CP2_HOSTNAME
      - $CP2_IP
    peerCertSANs:
      - $CP2_HOSTNAME
      - $CP2_IP

controllerManagerExtraArgs:
  node-monitor-grace-period: 10s
  pod-eviction-timeout: 10s

networking:
  podSubnet: 10.244.0.0/16
EOF

# 配置kubelet
kubeadm alpha phase certs all --config kubeadm-master.config
kubeadm alpha phase kubelet config write-to-disk --config kubeadm-master.config
kubeadm alpha phase kubelet write-env-file --config kubeadm-master.config
kubeadm alpha phase kubeconfig kubelet --config kubeadm-master.config
systemctl restart kubelet

# 添加etcd到集群中
KUBECONFIG=/etc/kubernetes/admin.conf kubectl exec -n kube-system etcd-${CP0_HOSTNAME} -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://${CP0_IP}:2379 member add ${CP2_HOSTNAME} https://${CP2_IP}:2380
kubeadm alpha phase etcd local --config kubeadm-master.config

# 部署
kubeadm alpha phase kubeconfig all --config kubeadm-master.config
kubeadm alpha phase controlplane all --config kubeadm-master.config
kubeadm alpha phase mark-master --config kubeadm-master.config

```

##### 4. 配置使用kubectl (在任意master节点操作)
```
rm -rf $HOME/.kube
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# 查看node节点
kubectl get nodes

# 只有网络插件也安装配置完成之后，才能会显示为ready状态
# 设置master允许部署应用pod，参与工作负载，现在可以部署其他系统组件
# 如 dashboard, heapster, efk等
kubectl taint nodes --all node-role.kubernetes.io/master-
```
##### 5. 配置网络插件 (在任意master节点操作)
```
curl -O https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

kubectl apply -f kube-flannel.yml
```

##### 6. 子节点加入集群 （所有子节点）
就是你初始化第一个master保存的命令。
```
kubeadm join k8s.test.local:8443 --token bqnani.kwxe3y34vy22xnhm --discovery-token-ca-cert-hash sha256:b6146fea7a63d3a66e406c12f55f8d99537db99880409939e4aba206300e06cc
```
##### 7. 查看节点状态
```
kubectl get nodes
```
返回如下：
```
NAME          STATUS    ROLES     AGE       VERSION
k8s-master1   Ready     master    49m       v1.11.1
k8s-master2   Ready     master    47m       v1.11.1
k8s-master3   Ready     master    45m       v1.11.1
k8s-slave1    Ready     <none>    42m       v1.11.1
k8s-slave2    Ready     <none>    42m       v1.11.1
```

##### 8. 配置k8s可视化
没有Git请自行安装。

```
# clone 配置文件
git clone https://github.com/BrickCat/kubernetes-dashboard.git

# 执行文件
cd kubernetes-dashboard

kubectl  -n kube-system create -f .

```
配置角色文件
```
vim dashboard-admin.yaml 
```
添加如下内容：
```
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: kubernetes-dashboard
  labels:
    k8s-app: kubernetes-dashboard
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: kubernetes-dashboard
  namespace: kube-system
```
```
kubectl -f ./dashboard-admin.yaml create
```

##### 9. 访问集群
 http://192.168.1.111:30090
 
 ![](http://pdjjmqkea.bkt.clouddn.com/HA.png)
